{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Introduction to Statistics](https://www.datacamp.com/completed/statement-of-accomplishment/course/0d1f2adcbc11306dd8f2e8dbb2539daf816d1cee)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/datacamp/blob/main/notebooks/data_science/introduction_to_statistics/notebook.ipynb)\n",
    "\n",
    "**Contents**\n",
    "- [Summary Statistics](#Summary-Statistics)\n",
    "- [Probability](#Probability)\n",
    "- [Distributions](#Distributions)\n",
    "- [Hypothesis Testing](#Hypothesis-Testing)\n",
    "- [Example](#Example)\n",
    "\n",
    "The field of statistics is the practice and study of collecting and analyzing data.\n",
    "\n",
    "There are two main branches:\n",
    "- **Descriptive**: describing or summarizing data\n",
    "- **Inferential**: collect a sample of data and apply the results to the population that the sample is from\n",
    "\n",
    "Statistics requires specific, measurable questions to be asked, like, \"Is rock music more popular than country?\" or \"Do women live longer than men?\"\n",
    "\n",
    "We can't use statistics to find out _why_ relationships exists, only _if_ they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "A _summary statistic_ is a fact or summary about data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Data\n",
    "\n",
    "Data can be _quantitative_ or _qualitative_.\n",
    "\n",
    "Quantitative data can be described as being either _discrete_ or _continuous_; and either _interval_ or _ratio_.\n",
    "\n",
    "Discrete data is data that can be counted, typically in whole numbers. For example, the number of children in a family is discrete data. Continuous data is data that can be measured, like the height of a person. Continuous data can be measured to any degree of precision.\n",
    "\n",
    "Interval data is numeric data that is evenly spaced (i.e., a consistent unit of measurement). Ratio data is interval data with a true zero point, which indicates the complete absence of what is being measured.\n",
    "\n",
    "Temperature is ratio data when measured in Kelvin, but interval when measured in Celsius or Fahrenheit. This is because 0 degrees Celsius does not indicate the complete absence of kinetic energy at the particle level.\n",
    "\n",
    "Qualitative or _categorical_ data can be nominal (unordered) or ordinal (ordered).\n",
    "\n",
    "Examples of nominal data are employment status or soccer player position. Examples of ordinal data are income status (e.g., low, middle, high) or customer satisfaction (e.g., unsatisfied, neutral, satisfied)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Center\n",
    "\n",
    "The three ways to calculate the \"center\" of a dataset are the _mean_ ($\\mu$), _median_ ($\\tilde{x}$), and _mode_:\n",
    "\n",
    "$\\mathrm{mean} = \\dfrac{\\textit{Sum of all values}}{\\textit{Count of all values}}$\n",
    "\n",
    "$\\mathrm{median} = \\textit{The middle value in a sorted dataset}$\n",
    "\n",
    "$\\mathrm{mode} = \\textit{The most common value in a dataset}$\n",
    "\n",
    "When data is symmetrical, the **mean** and **median** both work well. When the data is skewed, the **median** is a better measure of center because outliers _pull_ the mean towards them.\n",
    "\n",
    "The **mode** is useful for categorical data, for example, \"What city has the most robberies?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Spread\n",
    "\n",
    "Spread describes how far apart data points are. The three ways to calculate the \"spread\" of a dataset are the _range_, _variance_, and _standard deviation_ ($\\sigma$):\n",
    "\n",
    "$\\mathrm{range} = \\textit{Difference between the largest and smallest values}$\n",
    "\n",
    "$\\mathrm{variance} = \\dfrac{\\textit{Sum of all squared differences from the mean}}{\\textit{Count of all values}}$\n",
    "\n",
    "$\\mathrm{standard\\ deviation} = \\sqrt{\\mathrm{variance}}$\n",
    "\n",
    "The closer the standard deviation is to zero, the more the data is clustered around the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quartiles\n",
    "\n",
    "Quartiles are values that divide a dataset into four equal parts:\n",
    "- $Q_1$ is the first quartile, or the 25th percentile\n",
    "- $Q_2$ is the second quartile, or the 50th percentile\n",
    "- $Q_3$ is the third quartile, or the 75th percentile\n",
    "- $Q_4$ is the fourth quartile, or the 100th percentile\n",
    "\n",
    "$Q_2$ is always equal to the median.\n",
    "\n",
    "The interquartile range (IQR) is the difference between the third and first quartiles, or $Q_3 - Q_1$. The IQR is less affected by extreme values than other measures of spread.\n",
    "\n",
    "Quartiles are visualized using a _box plot_. The left-edge of the box is $Q_1$, the middle-line is $Q_2$ (median), and the right-edge is $Q_3$. The whiskers extend to the minimum and maximum values, and the outliers are plotted as individual points (dots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "Probability is how we measure _chance_.\n",
    "\n",
    "If we flip a coin, there is a 50% chance of getting heads, and a 50% chance of getting tails. If you roll a die, there is a 16.67% chance of getting a 1, 2, 3, 4, 5, or 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent vs Dependent Events\n",
    "\n",
    "Those are examples of _independent events_, where the outcome of one event does not affect the outcome of another event.\n",
    "\n",
    "_Conditional probability_ is how we calculate the chances of _dependent events_. If you draw a card from a deck, there is a 4/52 (7.7%) chance of drawing an Ace. If you have observed one Ace being drawn, then there is only a 3/51 (5.8%) chance of drawing another Ace after that observation.\n",
    "\n",
    "You can visualize conditional probability using a Venn diagram. The probability of event A given event B is the intersection of A and B divided by the probability of B. This is known as Bayes' Theorem:\n",
    "\n",
    "$P(A|B) = \\dfrac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "This says that, \"The probability of A given B is equal to the probability of A and B divided by the probability of B.\" Using the card example:\n",
    "\n",
    "$P(\\mathrm{Ace}|\\mathrm{Ace}) = \\dfrac{P(\\mathrm{Ace} \\cap \\mathrm{Ace})}{P(\\mathrm{Ace})} = \\dfrac{3/51 * 4/52}{4/52} = 3/51 = \\boxed{5.8\\%}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "A probability _distribution_ describes the probability of each possible outcome. The expected value of a distribution is the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution\n",
    "\n",
    "A _binomial distribution_ is a discrete distribution that measures the probability of a number of successes ($p$) in a sequence of independent events ($n$). The probabilities do not have to be evenly distributed.\n",
    "\n",
    "The expected value of a binomial distribution is $n * p$. For example, the expected number of heads out of 10 flips is $10 * 0.5 = \\boxed{5}$.\n",
    "\n",
    "If you don't know $p$ but you do know $n$ and $\\textit{expected value}$, then you can calculate $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distribution\n",
    "\n",
    "A _normal distribution_ (aka Guassian Distribution) is classified by its mean and standard deviation. The mean is the center of the distribution, and the standard deviation is the spread.\n",
    "\n",
    "A normal distribution is symmetric, meaning the curve is centered around the mean. The Empirical Rule (aka 3-sigma rule) states that 68.27% of the values fall within one standard deviation of the mean, 95.45% fall within two standard deviations, and 99.73% fall within three standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness\n",
    "\n",
    "_Skewness_ is a measure of the asymmetry of a distribution. A distribution is skewed if it is not symmetric. A left-skewed (aka negative) distribution has a long tail on the left side, and a right-skewed (aka positive) distribution has a long tail on the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis\n",
    "\n",
    "_Kurtosis_ is another way to describe the shape of a distribution by the size of its central peak and how spread out the tails are:\n",
    "* _leptokurtic_: high peak and thin tails.\n",
    "* _mesokurtic_: medium peak and medium tails.\n",
    "* _platykurtic_: low peak and wide tails.\n",
    "\n",
    "A distribution with _negative kurtosis_ has a flatter peak than a normal distribution, and a distribution with _positive kurtosis_ has a higher peak than a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Limit Theorem\n",
    "\n",
    "The _central limit theorem_ states that a distribution will approach a normal distribution as the size of a sample increases.\n",
    "\n",
    "It's important to note that the central limit theorem only applies when samples are taken randomly and not dependent. This is known as _Independent and Identically Distributed_ or _IID_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Law of Large Numbers\n",
    "\n",
    "The _Law of Large Numbers_ states that as the sample size increases, the sample mean converges to the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "The _Poisson distribution_ measures the probability of a number of events occurring in a fixed period of time. A _Poisson process_ is a process where the average number of events in a given time period is known, but the exact timing of events is random. For example, the average number of customers entering a store in a given hour is known, but the exact time that each customer enters is random.\n",
    "\n",
    "The distribution is represented by the value lambda ($\\lambda$), which is the average number of events per interval (this is also the expected value). In the store example, a lambda of 10 means that 10 customers are expected to enter the store in a given hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Mass Function\n",
    "\n",
    "The _probability mass function_ (PMF) is the probability of a specific number of events occurring in a given time period. The PMF is represented by the following formula:\n",
    "\n",
    "$P(X=k) = \\dfrac{e^{-\\lambda} \\lambda^k}{k!}$\n",
    "\n",
    "Where $k$ is the number of events, $\\lambda$ is the average number of events per interval, and $e$ is Euler's number ($e = 2.71828$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "_Hypothesis testing_ is a group of theories, methods, and techniques used to compare populations.\n",
    "\n",
    "In hypothesis testing, we always start with the position that no relationship exists between the variables or no difference between the groups. This is known as the _null hypothesis_ and is represented by $H_0$. The null is a statement about what we assume to be true until evidence suggests otherwise. The _alternative hypothesis_ is the opposite of the null hypothesis and is represented by $H_1$ or $H_a$.\n",
    "\n",
    "The _dependent variable_ in a hypothesis test is the variable that is being measured. The _independent variable_ is the variable that is being manipulated. On a scatter plot, the dependent variable is **always** on the y-axis.\n",
    "\n",
    "The dependent variable could be \"blood pressure\" and the independent variable could be \"medication dosage\". The null hypothesis would be that there is no difference in blood pressure between the different dosages of medication. The alternative hypothesis would be that more medication results in lower blood pressure.\n",
    "\n",
    "A hypothesis can be _1-tailed_ or _2-tailed_. A 1-tailed hypothesis is directional, meaning that it predicts the direction of the relationship between the variables. A 2-tailed hypothesis is non-directional, meaning that it does not predict the direction of the relationship between the variables. Using the blood pressure medication example, a 1-tailed hypothesis would be that more medication results in lower blood pressure, and a 2-tailed hypothesis would be that more medication results in a different blood pressure (higher or lower).\n",
    "\n",
    "There are many different types of hypothesis tests. Some of the most common are:\n",
    "  * _1-sample_: compares the mean of a sample to a known value\n",
    "  * _2-sample_: compares the means of two samples\n",
    "  * _ANOVA_: compares the means of three or more samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "_Experiments_ are a subset of hypothesis testing that involves performing tests on sample data to draw conclusions about the population.\n",
    "\n",
    "Experiments aim to answer a question in the form of, \"What is the effect of the _treatment_ on the _response_?\" The treatment refers to the independent variable, and the response refers to the dependent variable.\n",
    "\n",
    "If you were measuring the results of an advertising campaign, the advertisments would be the treatment, and the number of sales would be the response.\n",
    "\n",
    "The _randomized controlled trial_ or _RCT_ is the \"gold standard\" of experiments. In this type of experiment, participants are randomly assigned to either the control or treatment groups. The control group is not exposed to the treatment, and the treatment group is exposed to the treatment. The control group is used as a baseline to compare the results of the treatment group. An RCT can have many treatment groups, for example, different dosages of a medication. In software engineering, _A/B Testing_ is a common example of a randomized controlled trial, except in A/B testing, there are only ever 2 groups (A and B).\n",
    "\n",
    "A _blind trial_ can involve using a placebo to test the effectiveness of a treatment. In this type of experiment, the participants are not aware of which group they are in. This is used to prevent the _placebo effect_, where participants in the treatment group experience a positive effect from the treatment even though the treatment is not effective.\n",
    "\n",
    "A _double blind trial_ means the person administering the treatment is also unaware of which group the participants are in. This is used to prevent the _observer-expectancy effect_, where the experimenter's biases cause them to unconsciously influence the results of the experiment.\n",
    "\n",
    "An ideal experiment will minimize as much bias as possible. The groups should be as similar as possible, and the sample size should be as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "_Correlation_ is the way to measure relationships between variables. Remember, **correlation is not causation**!\n",
    "\n",
    "The _Pearson correlation coefficient_ is a measure of the strength of a linear relationship between two variables. It is represented by the letter $r$ and is always between -1 and 1. A value of 0 means there is no linear relationship between the variables.\n",
    "\n",
    "The sign indicates whether it is a positive or negative linear relationship. A _positive linear relationship_ means that as one variable increases, the other variable increases. A _negative linear relationship_ means that as one variable increases, the other variable decreases.\n",
    "\n",
    "A _linear relationship_ is a relationship where the change in one variable is proportional to the change in the other variable. For example, the number of hours worked is linearly related to the amount of money made (if you work more hours, you make more money). Measuring happiness is an example of a non-linear relationship, as happiness could increase quickly and then plateau (logarithmic). Linear relationships are important because they allow you to confidently predict the value of one variable based on the value of another variable.\n",
    "\n",
    "A _confounding variable_ is one that affects the dependent variable but is not the independent variable. For example, if you were measuring the effect of a new medication on blood pressure, the confounding variable could be the participant's diet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results with p-value\n",
    "\n",
    "The _p-value_ ($p$) is the probability of obtaining results at least as extreme as the observed results, assuming that the null hypothesis is correct. The p-value is always between 0 and 1.\n",
    "\n",
    "To reduce the risk of drawing a false conclusion, we set a _significance level_ or _alpha_ ($\\alpha$) before performing the experiment. The alpha is the probability of rejecting the null hypothesis and it is always between 0 and 1. A common value is `0.05` (5%). After data collection, if $p \\leq \\alpha$, we reject the null hypothesis. This is known as being _statistically significant_.\n",
    "\n",
    "The p-value is calculated in different ways depending on the type of test (1-sample, 2-sample, etc).\n",
    "\n",
    "The four potential conclusions of a hypothesis test are:\n",
    "- _False Positive_: The null hypothesis is rejected, but it is actually true (aka Type I error).\n",
    "- _False Negative_: The null hypothesis is accepted, but it is actually false (aka Type II error).\n",
    "- _True Positive_: The null hypothesis is rejected, and it is actually false.\n",
    "- _True Negative_: The null hypothesis is accepted, and it is actually true.\n",
    "\n",
    "The latter two are the correct conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We are testing a new drug that can reduce blood pressure.\n",
    "\n",
    "The null hypothesis is that the drug has no effect on blood pressure. The alternative hypothesis is that the drug reduces blood pressure (1-tailed).\n",
    "\n",
    "The participants are 20 adults with high blood pressure (140 mmHg) divided into control and treatment groups. They will take the drug daily over 8 weeks. Their blood pressure will be measured at the beginning and end of the study.\n",
    "\n",
    "The dependent variable is blood pressure. The independent variable is the drug. This is because the drug is being manipulated, and the blood pressure is being measured.\n",
    "\n",
    "Confounding variables could be diet, exercise, or age. The variable of interest is the change in blood pressure.\n",
    "\n",
    "The alpha is set to 0.05. The p-value is calculated using a 2-sample t-test.\n",
    "\n",
    "Based on the results, where $p = 0.024$, we reject the null hypothesis and conclude that the drug reduces blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test: 2.465\n",
      "p-value: 0.024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# small study to reduce variance with random numbers\n",
    "n = 20\n",
    "\n",
    "# reproducible seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# control group (c0) and treatment group (t0)\n",
    "c0 = np.random.randint(140, 160, n // 2)\n",
    "t0 = np.random.randint(140, 160, n // 2)\n",
    "\n",
    "# use normal distribution to simulate\n",
    "c1 = c0 + np.random.normal(-2, 5, n // 2)  # mean change of -2, stddev of 5\n",
    "t1 = t0 + np.random.normal(-10, 8, n // 2)  # mean change of -10, stddev of 8\n",
    "\n",
    "# results\n",
    "# only interested in the change, so multiplying by 100 isn't strictly necessary\n",
    "c = (c1 - c0) / c0 * 100\n",
    "t = (t1 - t0) / t0 * 100\n",
    "\n",
    "# t-test and p-val\n",
    "t_test, p_val = ttest_ind(c, t)\n",
    "\n",
    "print(f\"t-test: {t_test:.3f}\")\n",
    "print(f\"p-value: {p_val:.3f}\")  # below 0.05, so we reject the null hypothesis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
