{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Regular Expressions in Python](https://www.datacamp.com/completed/statement-of-accomplishment/course/43a09b72c90c13053dfdc04f32f46fbb77b25cbe)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/datacamp/blob/main/notebooks/courses/regular_expressions_in_python/notebook.ipynb)\n",
    "[![Render nbviewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/adamelliotfields/datacamp/blob/main/notebooks/courses/regular_expressions_in_python/notebook.ipynb)\n",
    "\n",
    "**Contents:**\n",
    "  * [String Manipulation](#string-manipulation)\n",
    "  * [String Formatting](#string-formatting)\n",
    "  * [Regular Expressions](#regular-expressions)\n",
    "  * [Advanced Regular Expressions](#advanced-regular-expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from string import Template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in this review: 135\n"
     ]
    }
   ],
   "source": [
    "movie = \"fox and kelley soon become bitter rivals because the new fox books store is opening up right across the block from the small business .\"\n",
    "statement = \"Number of characters in this review:\"\n",
    "\n",
    "# find number of characters\n",
    "length_string = len(movie)\n",
    "\n",
    "# convert to string\n",
    "to_string = str(length_string)\n",
    "\n",
    "# concatenate\n",
    "print(f\"{statement} {to_string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most significant tension of _rushmore_ is the potential relationship between a teacher and his student .\n",
      "the most significant tension of _rushmore_ is the potential relationship between a teacher and his student .\n"
     ]
    }
   ],
   "source": [
    "movie1 = \"the most significant tension of _election_ is the potential relationship between a teacher and his student .\"\n",
    "movie2 = \"the most significant tension of _rushmore_ is the potential relationship between a teacher and his student .\"\n",
    "# first 32 characters\n",
    "first_part = movie1[:32]\n",
    "\n",
    "# starting from 43rd character\n",
    "last_part = movie1[42:]\n",
    "\n",
    "# from 33rd to 42nd\n",
    "middle_part = movie2[32:42]\n",
    "\n",
    "# same!\n",
    "print(f\"{first_part}{middle_part}{last_part}\")\n",
    "print(movie2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts I stressed\n",
      "desserts I stressed\n"
     ]
    }
   ],
   "source": [
    "movie = \"oh my God! desserts I stressed was an ugly movie\"\n",
    "\n",
    "# get the word\n",
    "movie_title = movie[11:30]\n",
    "\n",
    "# get the palindrome\n",
    "palindrome = movie_title[::-1]\n",
    "\n",
    "# same!\n",
    "print(movie_title)\n",
    "print(palindrome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i supposed that coming from mtv films i should expect no less\n",
      "suppose\n"
     ]
    }
   ],
   "source": [
    "movie = \"$I supposed that coming from MTV Films I should expect no less$\"\n",
    "\n",
    "# convert to lowercase\n",
    "movie_lower = movie.lower()\n",
    "\n",
    "# remove `$`\n",
    "movie_no_sign = movie_lower.strip(\"$\")\n",
    "\n",
    "# split into substrings\n",
    "movie_split = movie_no_sign.split()\n",
    "\n",
    "# get 2nd word, all but last letter\n",
    "word_root = movie_split[1][:-1]\n",
    "print(movie_no_sign)\n",
    "print(word_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the film,however,is all good\n",
      "the film however is all good\n"
     ]
    }
   ],
   "source": [
    "movie = \"the film,however,is all good<\\\\i>\"\n",
    "\n",
    "# remove tags\n",
    "movie_tag = movie.strip(\"<\\\\i>\")\n",
    "\n",
    "# split on commas\n",
    "movie_no_comma = movie_tag.split(\",\")\n",
    "\n",
    "# join back\n",
    "movie_join = \" \".join(movie_no_comma)\n",
    "\n",
    "# print\n",
    "print(movie_tag)\n",
    "print(movie_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mtv films election', ' a high school comedy', ' is a current example']\n",
      "['from there', ' director steven spielberg wastes no time', ' taking us into the water on a midnight swim']\n"
     ]
    }
   ],
   "source": [
    "file = \"mtv films election, a high school comedy, is a current example\\nfrom there, director steven spielberg wastes no time, taking us into the water on a midnight swim\"\n",
    "\n",
    "# split at line boundaries (newline characters)\n",
    "file_split = file.split(\"\\n\")\n",
    "\n",
    "# split by commas\n",
    "for substring in file_split:\n",
    "    substring_split = substring.split(\",\")\n",
    "    print(substring_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word not found\n",
      "I believe you I always said that the actor is amazing in every movie he has played\n",
      "it's astonishing how frightening the actor norton looks with a shaved head and a swastika on his chest.\n"
     ]
    }
   ],
   "source": [
    "movies = pd.Series(\n",
    "    [\n",
    "        \"it's clear that he's passionate about his beliefs , and that he's not just a punk looking for an excuse to beat people up .\",\n",
    "        \"I believe you I always said that the actor actor actor is amazing in every movie he has played\",\n",
    "        \"it's astonishing how frightening the actor actor norton looks with a shaved head and a swastika on his chest.\",\n",
    "    ],\n",
    "    index=[200, 201, 202],\n",
    "    name=\"text\",\n",
    ")\n",
    "\n",
    "# if actor is not between char 37 and 41 (inclusive)\n",
    "for movie in movies:\n",
    "    if movie.find(\"actor\", 37, 42) == -1:\n",
    "        print(\"Word not found\")\n",
    "    # replace \"actor actor\" with \"actor\" only if repeated twice\n",
    "    elif movie.count(\"actor\") == 2:\n",
    "        print(movie.replace(\"actor actor\", \"actor\"))\n",
    "    # replace \"actor actor actor\" with \"actor\"\n",
    "    else:\n",
    "        print(movie.replace(\"actor actor actor\", \"actor\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "substring not found\n"
     ]
    }
   ],
   "source": [
    "movies = pd.Series(\n",
    "    [\n",
    "        \"heck , jackie doesn't even have enough money f...\",\n",
    "        \"in condor , chan plays the same character he's...\",\n",
    "    ],\n",
    "    index=[137, 138],\n",
    "    name=\"text\",\n",
    ")\n",
    "\n",
    "for movie in movies:\n",
    "    # find index of `money` between 12 and 50\n",
    "    try:\n",
    "        print(movie.index(\"money\", 12, 51))\n",
    "    except ValueError:\n",
    "        print(\"substring not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rest of the story is insignificant because all it does is serve as a mere backdrop for the two stars to share the screen .\n"
     ]
    }
   ],
   "source": [
    "movies = \"the rest of the story isn't important because all it does is serve as a mere backdrop for the two stars to share the screen .\"\n",
    "\n",
    "# replace negations\n",
    "movies_no_negation = movies.replace(\"isn't\", \"is\")\n",
    "\n",
    "# replace \"important\" with \"insignificant\"\n",
    "movies_antonym = movies_no_negation.replace(\"important\", \"insignificant\")\n",
    "\n",
    "# print\n",
    "print(movies_antonym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tool computer science is used in artificial intelligence\n",
      "The tool artificial intelligence is used in computer science\n"
     ]
    }
   ],
   "source": [
    "wikipedia_article = \"In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"\n",
    "my_list = []\n",
    "\n",
    "# characters 4 to 19 (inclusive)\n",
    "first_pos = wikipedia_article[3:19]\n",
    "\n",
    "# characters 22 to 44 (inclusive)\n",
    "second_pos = wikipedia_article[21:44]\n",
    "\n",
    "# template\n",
    "my_list.append(\"The tool {} is used in {}\")\n",
    "\n",
    "# rearrange template\n",
    "my_list.append(\"The tool {1} is used in {0}\")\n",
    "\n",
    "# print\n",
    "for my_string in my_list:\n",
    "    print(my_string.format(first_pos, second_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are interested in artificial intelligence, you can take the course related to neural networks\n"
     ]
    }
   ],
   "source": [
    "courses = [\"artificial intelligence\", \"neural networks\"]\n",
    "plan = {\n",
    "    \"field\": courses[0],\n",
    "    \"tool\": courses[1],\n",
    "}\n",
    "\n",
    "# use dict in template\n",
    "my_message = (\n",
    "    \"If you are interested in {plan[field]}, you can take the course related to {plan[tool]}\"\n",
    ")\n",
    "\n",
    "# the dict used becomes a keyword argument\n",
    "print(my_message.format(plan=plan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning. Today is September 30, 2023. It's 11:29 ... time to work!\n"
     ]
    }
   ],
   "source": [
    "# get today's date\n",
    "get_date = datetime.now()\n",
    "\n",
    "# add named placeholders with format specifiers\n",
    "message = \"Good morning. Today is {today:%B %d, %Y}. It's {today:%H:%M} ... time to work!\"\n",
    "\n",
    "# print\n",
    "print(message.format(today=get_date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-strings\n",
    "\n",
    "Proposed in [PEP-498](https://peps.python.org/pep-0498) and introduced in Python 3.6, _f-strings_ offer an elegant way to embed expressions inside string literals.\n",
    "\n",
    "_Format specifiers_ are preceded by a colon (`:`) and are used specify how the expression should be formatted. For example, you might want floats to only have 2 decimal places, or you might want strings to be quoted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science is considered 'sexiest job' in the 21st century\n",
      "About 2.500000e+18 of data is produced daily in the world\n",
      "Individuals create around 72.41% of the data but only 1.1% is analyzed\n"
     ]
    }
   ],
   "source": [
    "field1 = \"sexiest job\"\n",
    "field2 = \"data is produced daily\"\n",
    "field3 = \"Individuals\"\n",
    "fact1 = 21\n",
    "fact2 = 2500000000000000000\n",
    "fact3 = 72.41415415151\n",
    "fact4 = 1.09\n",
    "\n",
    "# complete f-strings\n",
    "print(f\"Data science is considered {field1!r} in the {fact1:d}st century\")\n",
    "print(f\"About {fact2:e} of {field2} in the world\")\n",
    "print(f\"{field3} create around {fact3:.2f}% of the data but only {fact4:.1f}% is analyzed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 tweets were downloaded in 7 minutes indicating a speed of 17.1 tweets per min\n",
      "www.datacamp.com\n",
      "Only 5.83% of the posts contain links\n"
     ]
    }
   ],
   "source": [
    "number1 = 120\n",
    "number2 = 7\n",
    "string1 = \"httpswww.datacamp.com\"\n",
    "list_links = [\n",
    "    \"www.news.com\",\n",
    "    \"www.google.com\",\n",
    "    \"www.yahoo.com\",\n",
    "    \"www.bbc.com\",\n",
    "    \"www.msn.com\",\n",
    "    \"www.facebook.com\",\n",
    "    \"www.news.google.com\",\n",
    "]\n",
    "\n",
    "# complete f-strings\n",
    "print(\n",
    "    f\"{number1} tweets were downloaded in {number2} minutes indicating a speed of {number1 / number2:.1f} tweets per min\"\n",
    ")\n",
    "print(f\"{string1.replace('https', '')}\")\n",
    "print(f\"Only {((len(list_links) * 100) / 120):.2f}% of the posts contain links\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price for a house in the east neighborhood was $1232443 in 04-20-2007\n",
      "The price for a house in the west neighborhood was $1432673 in 05-26-2006.\n"
     ]
    }
   ],
   "source": [
    "east = {\"date\": datetime(2007, 4, 20, 0, 0), \"price\": 1232443}\n",
    "west = {\"date\": datetime(2006, 5, 26, 0, 0), \"price\": 1432673}\n",
    "\n",
    "# complete f-strings\n",
    "print(\n",
    "    f\"The price for a house in the east neighborhood was ${east['price']} in {east['date']:%m-%d-%Y}\"\n",
    ")\n",
    "print(\n",
    "    f\"The price for a house in the west neighborhood was ${west['price']} in {west['date']:%m-%d-%Y}.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Strings\n",
    "\n",
    "The `string` module provides a `Template` class that supports `$`-based substitutions. This is useful for string templates that are provided by users and not known until runtime. For example, in an email app, you could let users store reusable templates.\n",
    "\n",
    "Template strings are safer because they do not evaluate expressions like f-strings do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Toolkit is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.\n",
      "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
      "Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.\n"
     ]
    }
   ],
   "source": [
    "tool1 = \"Natural Language Toolkit\"\n",
    "tool2 = \"TextBlob\"\n",
    "tool3 = \"Gensim\"\n",
    "description1 = \"suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.\"\n",
    "description2 = \"Python library for processing textual data. It provides a simple API for diving into common natural language processing tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\"\n",
    "description3 = \"robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.\"\n",
    "\n",
    "# create template\n",
    "wikipedia = Template(\"$tool is a $description\")\n",
    "\n",
    "# substitute and print\n",
    "print(wikipedia.substitute(tool=tool1, description=description1))\n",
    "print(wikipedia.substitute(tool=tool2, description=description2))\n",
    "print(wikipedia.substitute(tool=tool3, description=description3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are offering a 3-month beginner course on Natural Language Toolkit just for $20 monthly\n"
     ]
    }
   ],
   "source": [
    "tools = [\"Natural Language Toolkit\", \"20\", \"month\"]\n",
    "our_tool = tools[0]\n",
    "our_fee = tools[1]\n",
    "our_pay = tools[2]\n",
    "\n",
    "# create template\n",
    "course = Template(\"We are offering a 3-month beginner course on $tool just for $$$fee ${pay}ly\")\n",
    "\n",
    "# substitute and print\n",
    "print(course.substitute(tool=our_tool, fee=our_fee, pay=our_pay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check your answer 1: I really like the app. But there are some features that can be improved, and your answer 2: $answer2\n"
     ]
    }
   ],
   "source": [
    "answers = {\"answer1\": \"I really like the app. But there are some features that can be improved\"}\n",
    "\n",
    "# create template\n",
    "the_answers = Template(\"Check your answer 1: $answer1, and your answer 2: $answer2\")\n",
    "\n",
    "# use safe substution\n",
    "try:\n",
    "    print(the_answers.safe_substitute(answers))\n",
    "except KeyError:\n",
    "    print(\"Missing information\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "A _Regular Expression_ or _RegEx_ is a sequence of characters that define a search pattern. They are an extremely powerful tool for finding patterns in text.\n",
    "\n",
    "Regular expressions can be made up of regular characters and special _metacharacters_. Metacharacters have specific meaning and allow you to build complex expressions.\n",
    "\n",
    "When writing regular expressions, it is common to use Python's _raw strings_ to avoid having to escape backslashes. Raw strings are prefixed with an `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@robot9!', '@robot4&', '@robot9$', '@robot7%']\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = \"@robot9! @robot4& I have a good feeling that the show isgoing to be amazing! @robot9$ @robot7%\"\n",
    "\n",
    "# define regex\n",
    "regex = r\"@robot\\d\\W\"\n",
    "\n",
    "# find matches\n",
    "print(re.findall(regex, sentiment_analysis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User_mentions:2']\n",
      "['likes: 9']\n",
      "['number of retweets: 7']\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = \"Unfortunately one of those moments wasn't a giant squid monster. User_mentions:2, likes: 9, number of retweets: 7\"\n",
    "\n",
    "# find matches\n",
    "print(re.findall(r\"User_mentions:\\d\", sentiment_analysis))\n",
    "print(re.findall(r\"likes:\\s\\d\", sentiment_analysis))\n",
    "print(re.findall(r\"number\\sof\\sretweets:\\s\\d\", sentiment_analysis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is in love with scrappy.  He is missing him already\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = (\n",
    "    \"He#newHis%newTin love with$newPscrappy. #8break%He is&newYmissing him@newLalready\"\n",
    ")\n",
    "\n",
    "regex_sentence = r\"\\W\\dbreak\\W\"\n",
    "sentiment_sub = re.sub(regex_sentence, \" \", sentiment_analysis)\n",
    "regex_words = r\"\\Wnew\\w\"\n",
    "\n",
    "# print\n",
    "print(re.sub(regex_words, \" \", sentiment_sub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifiers\n",
    "\n",
    "_Quantifiers_ determine how many instances of a particular character or group must be present in order for a match to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.tellyourstory.com']\n",
      "['@blueKnight39']\n",
      "[]\n",
      "['@anitaLopez98', '@MyredHat31']\n",
      "['https://radio.foxnews.com']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pd.Series(\n",
    "    [\n",
    "        \"Boredd. Colddd @blueKnight39 Internet keeps stuffing up. Save me! https://www.tellyourstory.com\",\n",
    "        \"I had a horrible nightmare last night @anitaLopez98 @MyredHat31 which affected my sleep, now I'm really tired\",\n",
    "        \"im lonely  keep me company @YourBestCompany! @foxRadio https://radio.foxnews.com 22 female, new york\",\n",
    "    ],\n",
    "    index=[545, 546, 547],\n",
    "    name=\"text\",\n",
    ")\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "    # match http links and print\n",
    "    print(re.findall(r\"https?\\://\\S+\", tweet))\n",
    "\n",
    "    # match user mentions and print\n",
    "    print(re.findall(r\"@\\w+\\d+\", tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['32 minutes ago']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1st May 2019']\n",
      "[]\n",
      "[]\n",
      "['23rd June 2018']\n",
      "['23rd June 2018 17:54']\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pd.Series(\n",
    "    [\n",
    "        \"I would like to apologize for the repeated Video Games Live related tweets. 32 minutes ago\",\n",
    "        \"@zaydia but i cant figure out how to get there / back / pay for a hotel 1st May 2019\",\n",
    "        \"FML: So much for seniority, bc of technological ineptness 23rd June 2018 17:54\",\n",
    "    ],\n",
    "    index=[232, 233, 234],\n",
    "    name=\"text\",\n",
    ")\n",
    "\n",
    "for date in sentiment_analysis:\n",
    "    print(re.findall(r\"\\d{1,2}\\s\\w+\\sago\", date))\n",
    "    print(re.findall(r\"\\d{1,2}\\w+\\s\\w+\\s\\d{4}\", date))\n",
    "    print(re.findall(r\"\\d{1,2}\\w+\\s\\w+\\s\\d{4}\\s\\d{1,2}:\\d{2}\", date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITS', 'NOT', 'ENOUGH', 'TO', 'SAY', 'THAT', 'IMISS', 'U', '']\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = \"ITS NOT ENOUGH TO SAY THAT IMISS U #MissYou #SoMuch #Friendship #Forever\"\n",
    "\n",
    "# hashtag regex\n",
    "regex = r\"#\\w+\"\n",
    "\n",
    "# replace hashtags\n",
    "no_hashtag = re.sub(regex, \"\", sentiment_analysis)\n",
    "\n",
    "# get tokens by splitting text\n",
    "print(re.split(r\"\\s+\", no_hashtag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AIshadowhunters.txt']\n",
      " aaaaand back to my literature review. At least i have a friendly cup of coffee to keep me company\n",
      "['ouMYTAXES.txt']\n",
      " I am worried that I won't get my $900 even though I paid tax last year\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = [\n",
    "    \"AIshadowhunters.txt aaaaand back to my literature review. At least i have a friendly cup of coffee to keep me company\",\n",
    "    \"ouMYTAXES.txt I am worried that I won't get my $900 even though I paid tax last year\",\n",
    "]\n",
    "\n",
    "# match filenames that stat with 2 vowels\n",
    "regex = r\"^[aeiouAEIOU]{2,3}\\w+\\.txt\\b\"\n",
    "\n",
    "for text in sentiment_analysis:\n",
    "    # find files\n",
    "    print(re.findall(regex, text))\n",
    "\n",
    "    # replace all matches with empty string\n",
    "    print(re.sub(regex, \"\", text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email n.john.smith@gmail.com is a valid email\n",
      "The email 87victory@hotmail.com is a valid email\n",
      "The email !#mary-=@msca.net is invalid\n"
     ]
    }
   ],
   "source": [
    "emails = [\"n.john.smith@gmail.com\", \"87victory@hotmail.com\", \"!#mary-=@msca.net\"]\n",
    "\n",
    "# username can contain letters, numbers and some symbols\n",
    "# domain is separated by `@` and contains word characters followed by `.com`\n",
    "regex = r\"[A-Za-z0-9!#%&*\\$\\.]+@\\w+\\.com\"\n",
    "\n",
    "for example in emails:\n",
    "    if re.match(regex, example):\n",
    "        print(\"The email {email_example} is a valid email\".format(email_example=example))\n",
    "    else:\n",
    "        print(\"The email {email_example} is invalid\".format(email_example=example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The password Apple34!rose is a valid password\n",
      "The password My87hou#4$ is a valid password\n",
      "The password abc123 is invalid\n"
     ]
    }
   ],
   "source": [
    "passwords = [\"Apple34!rose\", \"My87hou#4$\", \"abc123\"]\n",
    "\n",
    "# passwords can contain letters, numbers, and some symbols\n",
    "# must be at least 8 characters, but no more than 20\n",
    "regex = r\"[A-Za-z0-9!#%&*\\$\\.]{8,20}\"\n",
    "\n",
    "for example in passwords:\n",
    "    # find a match\n",
    "    if re.search(regex, example):\n",
    "        # complete the format method to print out the result\n",
    "        print(\"The password {example} is a valid password\".format(example=example))\n",
    "    else:\n",
    "        print(\"The password {example} is invalid\".format(example=example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy vs Lazy\n",
    "\n",
    "_Greedy_ and _lazy_ describe how much text the regex engine should consume. Greedy matching tries to capture as as much text as possible, while lazy matching tries to capture as little text as possible.\n",
    "\n",
    "By default, quantifiers are greedy. You can make them lazy by appending a `?` to the quantifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to see that amazing show again!\n"
     ]
    }
   ],
   "source": [
    "string = \"I want to see that <strong>amazing show</strong> again!\"\n",
    "\n",
    "# no tags\n",
    "print(re.sub(r\"<.+?>\", \"\", string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['536', '12']\n",
      "['5', '3', '6', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = \"Was intending to finish editing my 536-page novel manuscript tonight, but that will probably not happen. And only 12 pages are left \"\n",
    "\n",
    "numbers_found_greedy = re.findall(r\"\\d+\", sentiment_analysis)\n",
    "numbers_found_lazy = re.findall(r\"\\d+?\", sentiment_analysis)\n",
    "\n",
    "print(numbers_found_greedy)\n",
    "print(numbers_found_lazy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying)\"]\n",
      "['(They were so cute)', \"(I'm crying)\"]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = \"Put vacation photos online (They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying). \"\n",
    "\n",
    "# match text in parens\n",
    "sentences_found_greedy = re.findall(r\"\\(.+\\)\", sentiment_analysis)\n",
    "sentences_found_lazy = re.findall(r\"\\(.+?\\)\", sentiment_analysis)\n",
    "\n",
    "print(sentences_found_greedy)\n",
    "print(sentences_found_lazy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "_Grouping_ allows you to treat multiple characters as a single unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists of users found in this tweet: ['statravelAU', 'statravelpo']\n",
      "Lists of users found in this tweet: ['Hollywoodheat34']\n",
      "Lists of users found in this tweet: ['msdrama098']\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = [\n",
    "    \"Just got ur newsletter, those fares really are unbelievable. Write to statravelAU@gmail.com or statravelpo@hotmail.com. They have amazing prices\",\n",
    "    \"I should have paid more attention when we covered photoshop in my webpage design class in undergrad. Contact me Hollywoodheat34@msn.net.\",\n",
    "    \"hey missed ya at the meeting. Read your email! msdrama098@hotmail.com\",\n",
    "]\n",
    "\n",
    "# capture the name part of the email\n",
    "regex_email = r\"([A-Za-z0-9]+)@\\S+\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "    # find all matches of regex in each tweet\n",
    "    email_matched = re.findall(regex_email, tweet)\n",
    "\n",
    "    # print\n",
    "    print(\"Lists of users found in this tweet: {}\".format(email_matched))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airline: IB Flight number: 3723\n",
      "Departure: AMS Destination: MAD\n",
      "Date: 06OCT\n"
     ]
    }
   ],
   "source": [
    "flight = \"Subject: You are now ready to fly. Here you have your boarding pass IB3723 AMS-MAD 06OCT\"\n",
    "\n",
    "# capture flight information\n",
    "regex = r\"([A-Z]{2})(\\d{4})\\s([A-Z]{3})-([A-Z]{3})\\s(\\d{2}[A-Z]{3})\"\n",
    "flight_matches = re.findall(regex, flight)\n",
    "\n",
    "# print\n",
    "print(\"Airline: {} Flight number: {}\".format(flight_matches[0][0], flight_matches[0][1]))\n",
    "print(\"Departure: {} Destination: {}\".format(flight_matches[0][2], flight_matches[0][3]))\n",
    "print(\"Date: {}\".format(flight_matches[0][4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments found [('love', 'concert', 'The Book of Souls World Tour')]\n",
      "Positive comments found [('enjoy', 'movie', 'Wreck-It Ralph')]\n",
      "Positive comments found [('like', 'movie', 'Wish Upon a Star')]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = [\n",
    "    \"I totally love the concert The Book of Souls World Tour. It kinda amazing!\",\n",
    "    \"I enjoy the movie Wreck-It Ralph. I watched with my boyfriend.\",\n",
    "    \"I still like the movie Wish Upon a Star. Too bad Disney doesn't show it anymore.\",\n",
    "]\n",
    "\n",
    "regex_positive = r\"(love|like|enjoy).+?(movie|concert)\\s(.+?)\\.\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "    positive_matches = re.findall(regex_positive, tweet)\n",
    "    print(\"Positive comments found {}\".format(positive_matches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-capturing Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments found [('dislike', 'The cabin and the ant')]\n",
      "Negative comments found [('disapprove', 'Honest with you')]\n",
      "Negative comments found [('dislike', 'After twelve Tour')]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = [\n",
    "    \"That was horrible! I really dislike the movie The cabin and the ant. So boring.\",\n",
    "    \"I disapprove the movie Honest with you. It's full of cliches.\",\n",
    "    \"I dislike very much the concert After twelve Tour. The sound was horrible.\",\n",
    "]\n",
    "\n",
    "# use `?:` for the non-capturing group\n",
    "regex_negative = r\"(hate|dislike|disapprove).+?(?:movie|concert)\\s(.+?)\\.\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "    negative_matches = re.findall(regex_negative, tweet)\n",
    "    print(\"Negative comments found {}\".format(negative_matches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backreferences\n",
    "\n",
    "A _backreference_ is a reference to a previously matched group. They can be numbered or named. When using numbered groups, the `0` group is the entire match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our first contract is dated back to 2001. Particularly, the day 25 of the month 03.\n"
     ]
    }
   ],
   "source": [
    "contract = \"Provider will invoice Client for Services performed within 30 days of performance.  Client will pay Provider as set forth in each Statement of Work within 30 days of receipt and acceptance of such invoice. It is understood that payments to Provider for services rendered shall be made in full as agreed, without any deductions for taxes of any kind whatsoever, in conformity with Providerâ€™s status as an independent contractor. Signed on 03/25/2001.\"\n",
    "\n",
    "# capture month, day, and year where the contract was **signed**\n",
    "regex_dates = r\"Signed\\son\\s(\\d{2})/(\\d{2})/(\\d{4})\"\n",
    "dates = re.search(regex_dates, contract)\n",
    "\n",
    "# create a dict from the results\n",
    "signature = {\n",
    "    \"day\": dates.group(2),\n",
    "    \"month\": dates.group(1),\n",
    "    \"year\": dates.group(3),\n",
    "}\n",
    "\n",
    "# print\n",
    "print(\n",
    "    \"Our first contract is dated back to {data[year]}. Particularly, the day {data[day]} of the month {data[month]}.\".format(\n",
    "        data=signature\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your tag body is closed\n",
      "Your tag article is closed\n",
      "Close your nav tag!\n"
     ]
    }
   ],
   "source": [
    "html_tags = [\n",
    "    \"<body>Welcome to our course! It would be an awesome experience</body>\",\n",
    "    \"<article>To be a data scientist, you need to have knowledge in statistics and mathematics</article>\",\n",
    "    \"<nav>About me Links Contact me!\",\n",
    "]\n",
    "\n",
    "for string in html_tags:\n",
    "    # find if there is a match\n",
    "    match_tag = re.match(r\"<(\\w+)>.*?</\\1>\", string)\n",
    "\n",
    "    if match_tag:\n",
    "        # if so, print the first captured group\n",
    "        print(\"Your tag {} is closed\".format(match_tag.group(1)))\n",
    "    else:\n",
    "        # if not, capture only the tag\n",
    "        notmatch_tag = re.match(r\"<(\\w+)>\", string)\n",
    "        print(\"Close your {} tag!\".format(notmatch_tag.group(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elongated word found: Moscooooooow\n",
      "Elongated word found: neighborrrrrrrs\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = [\n",
    "    \"@marykatherine_q i know! I heard it this morning and wondered the same thing. Moscooooooow is so behind the times\",\n",
    "    \"Staying at a friends house...neighborrrrrrrs are so loud-having a party\",\n",
    "    \"Just woke up an already have read some e-mail\",\n",
    "]\n",
    "\n",
    "# match elongated words\n",
    "regex_elongated = r\"\\w*(\\w)\\1\\w*\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "    match_elongated = re.search(regex_elongated, tweet)\n",
    "\n",
    "    if match_elongated:\n",
    "        elongated_word = match_elongated.group(0)\n",
    "        print(\"Elongated word found: {word}\".format(word=elongated_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = (\n",
    "    \"You need excellent python skills to be a data scientist. Must be! Excellent python\"\n",
    ")\n",
    "\n",
    "# positive lookahead\n",
    "look_ahead = re.findall(r\"\\w+(?=\\spython)\", sentiment_analysis)\n",
    "\n",
    "# positive lookbehind\n",
    "look_behind = re.findall(r\"(?<=[Pp]ython\\s)\\w+\", sentiment_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4564-646464-01']\n",
      "[]\n",
      "['6476-579052-01']\n",
      "[]\n",
      "['345-5785-544245']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cellphones = [\"4564-646464-01\", \"345-5785-544245\", \"6476-579052-01\"]\n",
    "\n",
    "# negative lookbehind\n",
    "for phone in cellphones:\n",
    "    number = re.findall(r\"(?<!\\d{3}-)\\d{4}-\\d{6}-\\d{2}\", phone)\n",
    "    print(number)\n",
    "\n",
    "# negative lookahead\n",
    "for phone in cellphones:\n",
    "    number = re.findall(r\"\\d{3}-\\d{4}-\\d{6}(?!-\\d{2})\", phone)\n",
    "    print(number)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacamp-kYionb3o-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
